{
    "collab_server" : "",
    "contents" : "# Some fun time series models {#funtimeseries}\n\n\n## This session\n\nIn this session, we'll cover two of the things that Stan lets you do quite simply: implement state\nspace models, and finite mixtures. \n\n\n### Finite mixtures\n\n\n\nIn a post [here](https://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html), \nI describe a simple model in which each observation of our data could have one of two densities. We estimated the \nparameters of both densities, and the probability of the data coming from either. While finite mixture models as in the\nlast post are a useful learning aid, we might want richer models for applied work. In particular, we might want the\nprobability of our data having each density to vary across observations. This is the first of two posts dedicated\nto this topic. I gave a [talk](https://dl.dropboxusercontent.com/u/63100926/become_a_bayesian_shareable.html) covering \nsome of this also (best viewed in Safari).\n\nFor sake of an example, consider this: the daily returns series of a stock has two states. In the first, the stock is 'priced to \nperfection', and so the price is an I(1) random walk (daily returns are mean stationary). In the second, there is momentum---here, \ndaily returns have AR(1) structure. Explicitly, for daily log returns $r_{t}$: \n\nState 1: $r_{t} \\sim \\mbox{normal}(\\alpha_{1}, \\sigma_{1})$\n\nState 2: $r_{t} \\sim \\mbox{normal}(\\alpha_{2} + \\rho_{1} r_{t-1}, \\sigma_{2})$\n\nWhen we observe a value of $r_{t}$, we don't know for sure whether it came from the first or second model--that is precisely \nwhat we want to infer. For this, we need a model for the probability that an observation came from each state $s_{t}\\in 1, 2$. One such model\ncould be: \n\n$$\n\\mbox{prob}(s_{t}=1 | \\mathcal{I}_{t}) = \\mbox{Logit}^{-1}(\\mu_{t})\n$$\n\nwith\n\n$$\n\\mu_{t} \\sim \\mbox{normal}(\\alpha_{3} + \\rho_{2}\\mu_{t-1} + f(\\mathcal{I}_{t}), \\sigma_{3})\n$$\n\nHere, $f(\\mathcal{I}_{t})$ is a function of the information available at the beginning of day $t$. If we had interesting\ninformation about sentiment, or news etc., it could go in here. For simplicity, let's say  $f(\\mathcal{I}_{t}) = \\beta r_{t-1}$. \n\nUnder this specification (and for a vector containing all parameters, $\\theta$), we can specify the likelihood contribution of \nan observation. It is simply the weighted average of likelihoods under each candidate data generating process, where the weights\nare the probabilities that the data comes from each density. \n\n\\[\np(r_{t} | \\theta) = \\mbox{Logit}^{-1}(\\mu_{t})\\, \\mbox{normal}(r_{t}|\\, \\alpha_{1}, \\sigma_{1}) + (1-\\mbox{Logit}^{-1}(\\mu_{t}))\\, \\mbox{normal}(r_{t}|\\, \\alpha_{2} + \\rho r_{t-1}, \\sigma_{2})\n\\]\n\nAs discussed in the last post, we work in log likelihoods, not likelihoods. This means we should use the `log_sum_exp()` function in \nStan. This means that we express the log likelihood contribution of a single point as:\n\n```{r, eval = F}\nlog_sum_exp(log(inv_logit(mu[t])) + normal_lpdf(r[t] | alpha[1], sigma[1]),\n            log((1 - inv_logit(mu[t]))) + normal_lpdf(r[t] | alpha[2] + rho[1], sigma[2]))\n```\n\nStan has recently added another function which performs the same calculation, but makes writing it out a bit easier. \nFor two log densities `lp1`, `lp2` and a mixing probability `theta`, we have\n\n```{r, eval = F}\nlog_mix(theta, lp1, lp2) = log_sum_exp(log(theta) + lp1,\n                                       log(1-theta) + lp2)\n```\n\n### Writing out the model\n\nThe Stan code for the model is: \n\n```{r, eval = F}\n// saved as time_varying_finite_mixtures.stan\ndata {\n  int T;\n  vector[T] r;\n}\nparameters {\n  vector[T] mu;\n  vector[2] rho;\n  real beta;\n  vector<lower = 0>[3] sigma;\n  vector[3] alpha; \n}\nmodel {\n  // priors \n  mu[1] ~ normal(0, .1);\n  sigma ~ cauchy(0, 0.5);\n  rho ~ normal(1, .1);\n  beta~ normal(.5, .25);\n  alpha[1:2] ~ normal(0, 0.1);\n  alpha[3] ~ normal(0, 1);\n\n  // likelihood\n  for(t in 2:T) {\n    mu[t] ~ normal(alpha[3] + rho[1]*mu[t-1] + beta* r[t-1], sigma[3]);\n\n    target += log_mix(inv_logit(mu[t]), \n                      normal_lpdf(r[t] | alpha[1], sigma[1]), \n                      normal_lpdf(r[t] | alpha[2] + rho[2] * r[t-1], sigma[2]));\n  }\n}\n```\n\n### Recapturing 'known unknowns'\n\nAs should be clear by now, I believe strongly that we should simulate from the model and make sure that \nwe can recapture \"known unknowns\" before taking the model to real data. Below we simulate some fake data. \n```{r, echo = F, message = F, warning = F}\nlibrary(rstan); library(ggplot2); library(dplyr); library(reshape2)\noptions(mc.cores = parallel::detectCores())\n```\n\n```{r, warning = F, message = F, cache = T}\n# Set some fake parameters\nalpha1 <- -0.01\nalpha2 <- 0.015\nrho1 <- 0.95\nrho2 <- 0.8\nbeta <- 0.5\n\nsigma1 <- 0.05\nsigma2 <- 0.03\nsigma3 <- 0.3\nT <- 500\nr <- rep(NA, T)\nr[1] <- 0\n\nmu <- rep(NA, T)\nz <- rep(NA, T)\nmu[1] <- 0\nz[1] <- 1\n\n\n# Simulate the data series\nfor(t in 2:T) {\n  mu[t]  <- rho1 * mu[t-1] + beta*(r[t-1]) + rnorm(1, 0, sigma3)\n  prob <- arm::invlogit(mu[t])\n  z[t] <- sample(1:2, 1, prob = c(prob, 1-prob))\n  \n  if(z[t]==1) {\n    # random walk state\n    r[t] <- rnorm(1, alpha1, sigma1)\n  } else {\n    # momentum state\n    r[t] <- rnorm(1, alpha2 + rho2*r[t-1], sigma2)\n  }\n}\n\n```\n\nYou should plot your data before doing anything. Let's take a look. \n\n```{r}\n# Plot the returns\nplot.ts(r)\n# Plot the probability of the random walk state\nplot.ts(arm::invlogit(mu))\n```\n\nLooks good! Now we compile and run the model. \n\n```{r, message = F, warning = F, results =\"hide\", cache = T}\ncompiled_model <- stan_model(\"time_varying_finite_mixtures.stan\")\n\nestimated_model <- sampling(compiled_model, data = list(r = r, T = T), cores = 4, chains = 4)\n```\n\nNow we inspect the parameter estimates, which should align with those in our data generating process. \n\n```{r}\nprint(estimated_model, pars = c(\"alpha\", \"rho\", \"sigma\"))\n```\n\nIt seems that most of the parameters appear to have estimated quite cleanly--most of the Rhats are fairly close, \nto 1, with the exception of the standard deviation of the updates in the latent series (which will be very \nweakly identified, given we don't observe `mu`). We would fix this by adding better prior information to the model. \n\n### Taking the model to real data\n\nNow we know that our program can recapture a known model, we can take it to some real data. In this case, we'll use\nthe log differences in sequential adjusted closing prices for Apple's common stock. With Apple being such a large, \nwell-researched (and highly liquid) stock, we should expect that it spends almost all time in the random walk state. \nLet's see what the data say! \n\n```{r, message = F, warning = F, cache = T}\n# Now with real data! \naapl <- Quandl::Quandl(\"YAHOO/AAPL\")\n\naapl <- aapl %>%\n  mutate(Date = as.Date(Date)) %>%\n  arrange(Date) %>% \n  mutate(l_ac = log(`Adjusted Close`),\n         dl_ac = c(NA, diff(l_ac))) %>% \n  filter(Date > \"2015-01-01\")\n\naapl_mod <- sampling(compiled_model, data= list(T = nrow(aapl), r = aapl$dl_ac*100))\n\n```\n\nNow check that the model has fit properly\n\n```{r, eval = F}\nshinystan::launch_shinystan(aapl_mod)\n```\n\nAnd finally plot the probability of being in each state.\n\n```{r, cache = T, warning = F, message = F}\nplot1 <- aapl_mod %>% \n  as.data.frame() %>% \n  select(contains(\"mu\")) %>%\n  melt() %>% \n  group_by(variable) %>% \n  summarise(lower = quantile(value, 0.95), \n            median = median(value),\n            upper = quantile(value, 0.05)) %>% \n  mutate(date = aapl$Date,\n         ac = aapl$l_ac) %>%\n  ggplot(aes(x = date)) + \n  geom_ribbon(aes(ymin = arm::invlogit(lower), ymax = arm::invlogit(upper)), fill= \"orange\", alpha = 0.4) +\n  geom_line(aes(y = arm::invlogit(median))) +\n  ggthemes::theme_economist() +\n  xlab(\"Date\") +\n  ylab(\"Probability of random walk model\")\n\n\nplot2 <- aapl_mod %>% \n  as.data.frame() %>% \n  select(contains(\"mu\")) %>%\n  melt() %>% \n  group_by(variable) %>% \n  summarise(lower = quantile(value, 0.95), \n            median = median(value),\n            upper = quantile(value, 0.05)) %>% \n  mutate(date = aapl$Date,\n         ac = aapl$`Adjusted Close`) %>%\n  ggplot(aes(x = date, y = ac)) +\n  geom_line() +\n  ggthemes::theme_economist() +\n  xlab(\"Date\") +\n  ylab(\"Adjusted Close\")\n\ngridExtra::grid.arrange(plot1, plot2)\n\n```\n\n\nAnd there we go! As expected, Apple spends almost all their time in the random walk state, \nbut, surprisingly, appears to have had a few periods with some genuine (mainly negative)\nmomentum. \n\n### Building up the model\n\nThe main problem with this model is that our latent state $\\mu$ can only really vary so much \nfrom period to period. That can delay the response to the appearance of a new state, and slow\nthe process of \"flipping back\" into the regular state. One way of getting around this is to have\na discrete state with more flexibility in flipping between states. We'll explore this in the next \npost, on Regime-Switching models. \n\n\n## A state space model involving polls\n\n\nThis tutorial covers how to build a low-to-high frequency interpolation\nmodel in which we have possibly many sources of information that occur\nat various frequencies. The example I'll use is drawing inference about\nthe preference shares of Clinton and Trump in the current presidential \ncampaign. This is a good example for this sort of imputation: \n\n- Data (polls) are sporadically released. Sometimes we have many released\nsimultaneously; at other times there may be many days with no releases. \n- The various polls don't necessarily agree. They might have different methodologies\nor sampling issues, resulting in quite different outcomes. We want to build\na model that can incorporate this. \n\nThere are two ingredients to the polling model. A multi-measurement model,\ntypified by Rubin's 8 schools example. And a state-space model. Let's briefly \ndescribe these. \n\n### Multi-measurement model and the 8 schools example\n\nLet's say we run a randomized control trial in 8 schools. Each school $i$ reports\nits own treatment effect $te_{i}$, which has a standard error $\\sigma_{i}$. There\nare two questions the 8-schools model tries to answer: \n\n- If you administer the experiment at one of these schools, say, school 1, and have your estimate \nof the treatment effect $te_{1}$, what do you expect would be the treatment effect if \nyou were to run the experiment again? In particular, would your expectations of the \ntreatment effect in the next experiment change once you learn the treatment effects estimated\nfrom the experiments in the other schools? \n- If you roll out the experiment at a new school (school $9$), what do we expect the \ntreatment effect to be? \n\nThe statistical model that Rubin proposed is that each school has its own _true_ \nlatent treatment effect $y_{i}$, around which our treatment effects are distributed.\n\n$$\nte_{i} \\sim \\mathcal{N}(y_{i}, \\sigma_{i})\n$$\n\nThese \"true\" but unobserved treatment effects are in turn distributed according to \na common hyper-distribution with mean $\\mu$ and standard deviation $\\tau$\n\n$$\ny_{i} \\sim \\mathcal{N}(\\mu, \\tau)\n$$\n\nOnce we have priors for $\\mu$ and $\\tau$, we can estimate the above model with Bayesian \nmethods. \n\n\n### A state-space model\n\nState-space models are a useful way of dealing with noisy or incomplete data,\nlike our polling data. The idea is that we can divide our model into two parts:\n\n- **The state**. We don't observe the state; it is a latent variable. But we know\nhow it changes through time (or at least how large its potential changes are).\n- **The measurement**. Our state is measured with imprecision. The measurement\nmodel is the distribution of the data that we observe around the state. \n\nA simple example might be consumer confidence, an unobservable latent construct\nabout which our survey responses should be distributed. So our state-space model would be:\n\nThe state\n\n$$\nconf_{t} \\sim \\mathcal{N}(conf_{t-1}, \\sigma)\n$$\n\nwhich simply says that consumer confidence is a random walk with normal innovations\nwith a standard deviation $\\sigma$, and \n\n$$\n\\mbox{survey_measure}_{t} \\sim \\mathcal{N}(conf_{t}, \\tau)\n$$\n\nwhich says that our survey measures are normally distributed around the true latent \nstate, with standard deviation $\\tau$. \n\nAgain, once we provide priors for the initial value of the state $conf_{0}$ and $\\tau$, \nwe can estimate this model quite easily. \n\nThe important thing to note is that we have a model for the state even if there\nis no observed measurement. That is, we know (the distribution for) how consumer confidence should progress\neven for the periods in which there are no consumer confidence surveys. This makes\nstate-space models ideal for data with irregular frequencies or missing data. \n\n### Putting it together\n\nAs you can see, these two models are very similar: they involve making inference\nabout a latent quantity from noisy measurements. The first shows us how we can aggregate \nmany noisy measurements together _within a single time period_, while the second\nshows us how to combine irregular noisy measures _over time_. We can now combine\nthese two models to aggregate multiple polls over time. \n\nThe data generating process I had in mind is a very simple model where each candidate's\npreference share is an unobserved state, which polls try to measure. Unlike some\nvolatile poll aggregators, I assume that the unobserved state can move according\nto a random walk with normal disturbances of standard deviation .25%. This greatly\nsmoothes out the sorts of fluctuations we see around the conventions etc. We could \nestimate this parameter using fairly tight priors, but I just hard-code it in for simplicity. \n\nThat is, we have the state for candidate $c$ in time $t$ evolving according to\n\n$$\n\\mbox{Vote share}_{c, t} \\sim \\mathcal{N} (\\mbox{Vote share}_{c, t-1}. 0.25)\n$$\n\nwith measurements being made of this in the polls. Each poll $p$ at time $t$ is\ndistributed according to \n\n$$\n\\mbox{poll}_{c, p, t} \\sim \\mathcal{N} (\\mbox{Vote share}_{c, t}. \\tau)\n$$\n\nI give an initial state prior of 50% to Clinton and a 30% prior to Trump May of last year. As we get further\nfrom that initial period, the impact of the prior is dissipated. \n\nThe code to download the data, run the model is below. You will need to have the most recent version of ggplot2 installed.\n\n```\n// saved as models/state_space_polls.stan\n\ndata {\n  int polls; // number of polls\n  int T; // number of days\n  matrix[T, polls] Y; // polls\n  matrix[T, polls] sigma; // polls standard deviations\n  real initial_prior;\n}\nparameters {\n  vector[T] mu; // the mean of the polls\n  real<lower = 0> tau; // the standard deviation of the random effects\n  matrix[T, polls] shrunken_polls;\n}\nmodel {\n  // prior on initial difference\n  mu[1] ~ normal(initial_prior, 1);\n  tau ~ student_t(4, 0, 5);\n  // state model\n  for(t in 2:T) {\n    mu[t] ~ normal(mu[t-1], 0.25);\n  }\n  \n  // measurement model\n  for(t in 1:T) {\n    for(p in 1:polls) {\n      if(Y[t, p] != -9) {\n        Y[t,p]~ normal(shrunken_polls[t, p], sigma[t,p]);\n        shrunken_polls[t, p] ~ normal(mu[t], tau);\n      } else {\n        shrunken_polls[t, p] ~ normal(0, 1);\n      }\n    }\n  }\n}\n\n```\n\n```{r, results = \"hide\", message = F, warning = F}\nlibrary(rvest); library(dplyr); library(ggplot2); library(rstan); library(reshape2); library(stringr); library(lubridate)\noptions(mc.cores = parallel::detectCores())\nsource(\"models/theme.R\")\n\n# The polling data\nrealclearpolitics_all <- read_html(\"http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html#polls\")\n\n# Scrape the data\npolls <- realclearpolitics_all %>% \n  html_node(xpath = '//*[@id=\"polling-data-full\"]/table') %>% \n  html_table() %>% \n  filter(Poll != \"RCP Average\")\n\n# Function to convert string dates to actual dates\nget_first_date <- function(x){\n  last_year <- cumsum(x==\"12/22 - 12/23\")>0\n  dates <- str_split(x, \" - \")\n  dates <- lapply(1:length(dates), function(x) as.Date(paste0(dates[[x]], \n                                                              ifelse(last_year[x], \"/2015\", \"/2016\")), \n                                                       format = \"%m/%d/%Y\"))\n  first_date <- lapply(dates, function(x) x[1]) %>% unlist\n  second_date <- lapply(dates, function(x) x[2])%>% unlist\n  data_frame(first_date = as.Date(first_date, origin = \"1970-01-01\"), \n             second_date = as.Date(second_date, origin = \"1970-01-01\"))\n}\n\n# Convert dates to dates, impute MoE for missing polls with average of non-missing, \n# and convert MoE to standard deviation (assuming MoE is the full 95% one sided interval length??)\npolls <- polls %>% \n  mutate(start_date = get_first_date(Date)[[1]],\n         end_date = get_first_date(Date)[[2]],\n         N = as.numeric(gsub(\"[A-Z]*\", \"\", Sample)),\n         MoE = as.numeric(MoE))%>% \n  select(end_date, `Clinton (D)`, `Trump (R)`, MoE) %>% \n  mutate(MoE = ifelse(is.na(MoE), mean(MoE, na.rm = T), MoE),\n         sigma = MoE/2) %>% \n  arrange(end_date) %>% \n  filter(!is.na(end_date))\n\n\n# Stretch out to get missing values for days with no polls\npolls3 <- left_join(data_frame(end_date = seq(from = min(polls$end_date), \n                                              to= as.Date(\"2016-08-04\"), \n                                              by = \"day\")), polls) %>% \n  group_by(end_date) %>%\n  mutate(N = 1:n()) %>%\n  rename(Clinton = `Clinton (D)`,\n         Trump = `Trump (R)`)\n\n\n# One row for each day, one column for each poll on that day, -9 for missing values\nY_clinton <- polls3 %>% dcast(end_date ~ N, value.var = \"Clinton\") %>% \n  dplyr::select(-end_date) %>% \n  as.data.frame %>% as.matrix\nY_clinton[is.na(Y_clinton)] <- -9\n\nY_trump <- polls3 %>% dcast(end_date ~ N, value.var = \"Trump\") %>% \n  dplyr::select(-end_date) %>% \n  as.data.frame %>% as.matrix\nY_trump[is.na(Y_trump)] <- -9\n\n# Do the same for margin of errors for those polls\nsigma <- polls3 %>% dcast(end_date ~ N, value.var = \"sigma\")%>% \n  dplyr::select(-end_date)%>% \n  as.data.frame %>% as.matrix\nsigma[is.na(sigma)] <- -9\n\n# Run the two models\n\nclinton_model <- stan(\"models/state_space_polls.stan\", \n                      data = list(T = nrow(Y_clinton), \n                                  polls = ncol(Y_clinton), \n                                  Y = Y_clinton, \n                                  sigma = sigma,\n                                  initial_prior = 50), iter = 600)\n\n\ntrump_model <- stan(\"models/state_space_polls.stan\", \n                    data = list(T = nrow(Y_trump), \n                                polls = ncol(Y_trump), \n                                Y = Y_trump, \n                                sigma = sigma,\n                                initial_prior = 30), iter = 600)\n\n\n\n# Pull the state vectors\n\nmu_clinton <- extract(clinton_model, pars = \"mu\", permuted = T)[[1]] %>% \n  as.data.frame\n\nmu_trump <- extract(trump_model, pars = \"mu\", permuted = T)[[1]] %>% \n  as.data.frame\n\n# Rename to get dates\nnames(mu_clinton) <- unique(paste0(polls3$end_date))\nnames(mu_trump) <- unique(paste0(polls3$end_date))\n\n\n# summarise uncertainty for each date\n\nmu_ts_clinton <- mu_clinton %>% melt %>% \n  mutate(date = as.Date(variable)) %>% \n  group_by(date) %>% \n  summarise(median = median(value),\n            lower = quantile(value, 0.025),\n            upper = quantile(value, 0.975),\n            candidate = \"Clinton\")\n\nmu_ts_trump <- mu_trump %>% melt %>% \n  mutate(date = as.Date(variable)) %>% \n  group_by(date) %>% \n  summarise(median = median(value),\n            lower = quantile(value, 0.025),\n            upper = quantile(value, 0.975),\n            candidate = \"Trump\")\n\n# Plot results\n\n\nbind_rows(mu_ts_clinton, mu_ts_trump) %>% \n  ggplot(aes(x = date)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, fill = candidate),alpha = 0.1) +\n  geom_line(aes(y = median, colour = candidate)) +\n  ylim(30, 60) +\n  scale_colour_manual(values = c(\"blue\", \"red\"), \"Candidate\") +\n  scale_fill_manual(values = c(\"blue\", \"red\"), guide = F) +\n  geom_point(data = polls3, aes(x = end_date, y = `Clinton`), size = 0.2, colour = \"blue\") +\n  geom_point(data = polls3, aes(x = end_date, y = Trump), size = 0.2, colour = \"red\") +\n  theme_lendable() + # Thanks to my employer for their awesome theme!\n  xlab(\"Date\") +\n  ylab(\"Implied vote share\") +\n  ggtitle(\"Poll aggregation with state-space smoothing\", \n          subtitle= paste(\"Prior of 50% initial for Clinton, 30% for Trump on\", min(polls3$end_date)))\n\n```",
    "created" : 1484823016266.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "477193267",
    "id" : "415AD6FF",
    "lastKnownWriteTime" : 1484916341,
    "last_content_update" : 1484916341880,
    "path" : "~/Documents/BSEcon/Econ short course/03-fun_time_series_models.Rmd",
    "project_path" : "03-fun_time_series_models.Rmd",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}