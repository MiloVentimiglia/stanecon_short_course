{
    "collab_server" : "",
    "contents" : "# An introduction to hierarchical modeling {#hierarchical}\n\n\n### What is hierarchical modeling\n\nHierarchical modeling is the practice of building _rich_ models, typcially in which each individual in your dataset \nhas their own set of parameters. Of course, without good prior information, this might not be identified, or might\nbe very weakly identified. Hierarchical modeling helps us deal with this problem by considering parameters at the low\nlevel as \"sharing\" information across individuals. This structure is known as \"partial pooling\". This session covers\npartial pooling, starting from the canonical example \"8 schools\", then shows how you can use partial pooling to provide\nprior information when combining previous studies with a new dataset. Finally I show how partial pooling can be used \nfor analysis of panel data. \n\n### Why do hierarchical modeling? \n\nThere are a few excellent reasons to do hierarchical modeling:\n\n**To deal with unobserved information fairly fixed at the level of the group**\n\nThe standard reason in economics to use panel data is to be able to \"control for\" confounding information that is fixed at the \nlevel of the individual over time. A similar motivation exists in hierarchical modeling. \n\nThe big difference is that we will not consider the individual or time effects to be fixed. Indeed, we routinely \n\"shrink\" effects towards a group-level average. This encodes the heuristic \"death, taxes, and mean reversion\". Cross-validating\nyour results will almost always show that such an approach is superior to fixed effects for prediction. \n\n**Prediction with high-dimensional categorical variables**\n\nOften in applied economics we have very high-dimensional categorical variables. For instance, plant, manager, project etc. \nThis can massively increase the size of the parameter space, and result in over-fitting/poor generalization. In contrast, \nimplementing high-dimensioned predictors as (shrunken) random effects typically results in large improvements in predictive\npower. \n\n**Mr P: Multi-level regression and post-stratification**\n\nAs economists we want to make inferences, typically of a causal nature. A common problem is that our data are not \ncollected randomly; we have some survey bias. Frequentists tend to correct for this by weighting observations according\nto the inverse of their probability of being observed. Yet this approach moves away from a generative model, making \nmodel comparison and validation difficult. \n\nMr P is the practice of fitting a model in which individuals, or groups of individuals (say, grouped by demographic cell)\nhave their own sets of parameters (which are shrunk towards a hierarchical prior). When we want to make an inference for a new population\nwe only need to know its demographics. The inference is the weighted average across effects in the sample, with weights coming\nfrom the new population. \n\nThis method has the advantage that we can work with highly-biased samples, while keeping within a generative framework (making\nModern Statistical Workflow completely doable). In a notorious example, Mr P was used by David Rothschild at Microsoft Research\nto predict the outcome of the 2012 election based on a survey run through the Xbox platform. The survey was almost entirely young men. \n\n\n\n\n### Exchangeability\n\nAstute readers familiar with fixed effects models will have noted a problem with one of my arguments above. I said that \nwe could use random intercepts to soak up unobserved information that affects both $X$ and $y$ by including group-varying\nintercepts $\\alpha_{j}$. But this implies that the unobserved information fixed in a group, $\\alpha_{j}$, is correlated with \n$X$. This correlation violates a very important rule in varying-intercept, varying-slope models: exchangeability. \n\n> Exchangeability says that there should be no information other than the outcome $y$ that should allow us to distinguish the \ngroup to which a group-level parameter belongs.\n\nIn this example, we can clearly use values of X to predict $j$, violating exchangeability. But all is not lost. The group-varying\nparameter needs not be uncorrelated with X, _only the random portion of it_. \n\n\n### Conditional exchangeability and the Bafumi Gelman correction\n\nImagine we have an exchangability issue for a very simple model with only group-varying intercept: \nthe unobserved information $\\alpha_{j}$ is correlated with $X_{i,j}$ across groups. \nLet's break $\\alpha_{j}$ down into its fixed and random portions. \n\n$$\n\\alpha_{j} = \\mu_{1} + \\eta_{j}\n$$\n\nwhere\n$$\n\\eta_{j} \\sim \\mbox{normal}(0, \\sigma_{\\eta})\n$$\n\nSo that now the regression model can be written as\n\n$$\ny_{i,t} = \\mu_{1}  + X_{i,j}\\beta + e_{i,j} \\mbox{ where } e_{i,j} = \\epsilon_{i,j}+ \\eta_{j}\n$$\n\nFor the correlation to hold, it must be the case that $\\eta_{j}$ is correlated with $X_{i,j}$. But our regression error is \n$e_{i,j}$, which is clearly correlated with $X$ violating the Gauss-Markov theorem and so giving us biased estimates. \n\nIn a [nice little paper](http://www.stat.columbia.edu/~gelman/research/unpublished/Bafumi_Gelman_Midwest06.pdf) Bafumi and Gelman suggest an elegant fix to this: simply control for group level averages in the model of $\\alpha_{j}$. This is a Bayesian take \non what econometricians might know as a Mundlak/Chamberlain approach. If $\\bar{X}_{j}$ is the mean of $X_{i,j}$ in group $j$, \nthen we could use the model \n\n$$\n\\alpha_{j} = \\hat{\\alpha} + \\gamma \\hat{X}_{j} + \\nu_{j}\n$$\n\nwhich results in the correlaton between $\\nu_{j}$ and $X_{i,j}$ across groups being 0. It's straightforward to implement, and\ngets you to _conditional exchangeability_---a condition under which mixed models like this one are valid. \n\n\n### Exercise 1: Hierarchical priors\n\nIn this exercise we'll estimate an experimental treatment effect using linear\nregression, while incorporating prior information from previous studies. Rather than doing this in stages \n(estimating the treatment effect and then doing some meta-analysis), we'll do everything in one pass. \nThis has the advantage of helping us to get more precise estimates of all our model parameters. \n\n### A very basic underlying model\n\nLet's say that we run the $J$'th experiment estimating the treatment effect of some treatment $x$ on an outcome $Y$. \nIt's an expensive and ethically challenging experiment to run, so unfortunately we're only able to get a sample size of 20.\nFor simplicity, we can assume that the treatment has the same treatment effect for all people, $\\theta$ (this is easily\ndropped in more elaborate analyses). There have been $J-1$ similar experiments \nrun in the past. In this example our outcome data $Y$ are conditionally normally distributed\nwith (untreated) mean $\\mu$ and standard deviation $\\sigma$. There is nothing to stop us from having a far\nmore complex model for the data. So the outcome model looks like this: \n\n$$\ny_{i, J} \\sim \\mbox{Normal}(\\mu + \\theta_{J} x_{i,J}, \\sigma)\n$$\n\nThe question is: how can we estimate the parameters of this model while taking account of \nthe information from the $J-1$ previous studies? The answer is to use the so-called _hierarchical prior_. \n\n### The hierarchical prior\n\nLet's say that each of the $J-1$ previous studies each has an estimated treatment effect $\\beta_{j}$, estimated \nwith some standard error $se_{j}$. Taken together, are these estimates of $\\beta_{j}$ the ground truth for the true \ntreatment effect in their respective studies? One way of answering this is to ask ourselves: if the researchers\nof each of those studies replicated their study in precisely the same way, but _after_ checking the estimates estimated \nby the other researchers, would they expect to find the same estimate they found before, $\\beta_{j}$? Or would \nthey perhaps expect some other treatment effect estimate, $\\theta_{j}$, that balances the information from their own\nstudy with the other studies? \n\nThe answer to this question gives rise to the hierarchical prior. In this prior, we say that the estimated treatment effect $\\beta$\nis a noisy measure of the underlying treatment effect $\\theta_{j}$ for each study $j$. These underlying effects\nare in turn noisy estimates of the true average treatment effect $\\hat{\\theta}$---noisy because of uncontrolled-for varation across experiments. That is, if we make assumptions of normality:\n\n$$\n\\beta_{j} \\sim \\mbox{Normal}(\\theta_{j}, se_{j})\n$$\n\nand\n\n$$\n\\theta_{j} \\sim \\mbox{Normal}\\left(\\hat{\\theta}, \\tau\\right)\n$$\n\nWhere $\\tau$ is the standard devation of the distribution of plausible experimental estimates. \n\nThe analysis therefore has the following steps: \n\n- Build a model of the treatment effects, considering our own study as another data point\n- Jointly estimate the hyperdistribution of treatment effects. \n\nAs an example, we'll take the original 8-schools data, with some fake data for the experiment we want to estimate. \nThe 8-schools example comes from an education intervention modeled by Rubin, in which a similar experiment was \nconducted in 8 schools, with only treatment effects and their standard errors reported. The task is to generate\nan estimate of the possible treatment effect that we might expect if we were to roll out the program across all schools. \n\n```{r, warning = F, message = F}\nlibrary(rstan); library(dplyr); library(ggplot2); library(reshape2)\n\n# The original 8 schools data\nschools_dat <- data_frame(beta = c(28,  8, -3,  7, -1,  1, 18, 12),\n                          se = c(15, 10, 16, 11,  9, 11, 10, 18))\n\n# The known parameters of our data generating process for fake data\nmu <- 10\nsigma <- 5\nN <- 20\n# Our fake treatment effect estimate drawn from the posterior of the 8 schools example\ntheta_J <- rnorm(1, 8, 6.45) \n\n# Create some fake data\ntreatment <- sample(0:1, N, replace = T)\ny <- rnorm(N, mu + theta_J*treatment, sigma)\n```\n\nThe Stan program we use to estimate the model is below. Note that these models can be difficult to fit, \nand so we employ a \"reparameterization\" below for the `theta`s. This is achieved by noticing that if \n\n$$\n\\theta_{j} \\sim \\mbox{Normal}\\left(\\hat{\\theta}, \\tau\\right)\n$$\nthen \n$$\n\\theta_{j} = \\hat{\\theta} + \\tau z_{j}\n$$\n\nwhere $z_{j}\\sim\\mbox{Normal}(0,1)$. A standard normal has an easier geometry for Stan to work with, so\nthis parameterization of the model is typically preferred. Here is the Stan model:\n\n```{r, eval = F}\n// We save this as 8_schools_w_regression.stan\ndata {\n  int<lower=0> J; // number of schools \n  int N; // number of observations in the regression problem\n  real beta[J]; // estimated treatment effects from previous studies\n  real<lower=0> se[J]; // s.e. of those effect estimates \n  vector[N] y; // the outcomes for students in our fake study data\n  vector[N] treatment; // the treatment indicator in our fake study data\n}\nparameters {\n  real mu; \n  real<lower=0> tau;\n  real eta[J+1];\n  real<lower = 0> sigma;\n  real theta_hat;\n}\ntransformed parameters {\n  real theta[J+1];\n  for (j in 1:(J+1)){\n    theta[j] = theta_hat + tau * eta[j];\n  }\n}\nmodel {\n  // priors\n  tau ~ cauchy(5, 2);\n  mu ~ normal(10, 2);\n  eta ~ normal(0, 1);\n  sigma ~ cauchy(3, 3);\n  theta_hat ~ normal(8, 5);\n  \n  // parameter model for previous studies\n  for(j in 1:J) {\n    beta[j] ~ normal(theta[j], se[j]);\n  }\n  \n  // our regression\n  y ~ normal(mu + theta[J+1]*treatment, sigma);\n  \n}\n\n```\n\nNow we estimate the model from R. Because of the geometry issues mentioned above, we use \n`control = list(adapt_delta = 0.99)` to prompt Stan to take smaller step sizes, improving \nsampling performance at a cost of slower estimation time (this isn't a problem here; it \nestimates in a couple of seconds). \n\n```{r, results = \"hide\", message = F, warning = F}\neight_schools_plus_regression <- stan(\"8_schools_w_regression.stan\",\n                       data = list(beta = schools_dat$beta,\n                                   se = schools_dat$se,\n                                   J = 8,\n                                   y = y,\n                                   N = N,\n                                   treatment = treatment),\n                       control = list(adapt_delta = 0.99))\n```\n\nLet's comapare the estimates we get for our regression model to those we might get from \nthe Bayesian model. A simple linear regression model gives us the following confidence intervals\nfor the parameter estimates: \n\n```{r, echo = F, results = \"asis\", message = F, warning = F}\ndata_frame(coef = c(\"mu\", \"theta[9]\"), estimates = coef(lm(y ~ treatment))) %>% bind_cols(confint(lm(y ~ treatment)) %>% as.data.frame) %>% pander::pander()\n```\n\nOur Bayesian model gives us more precise estimates for the treatment effect, with the 95% credibility region considerably smaller. \nThis is because we have \"borrowed\"\" information from the previous studies when estimating the treatment effect in the latest study. The estimates are also closer to the group-level mean.\n\n```{r}\nprint(eight_schools_plus_regression, pars = c(\"mu\", \"theta[9]\", \"theta_hat\"), probs = c(0.025, 0.5, 0.975))\n```\n\n### A note on reparameterizing\n\nHierarchical models are famous for inducing regions of high curvature in the typical set (see Betancourt 2017). Often, if we\nimplement these directly we get many divergent transitions, in which we cannot trust the results. We often use a reparameterization\nto reshape the posterior into one that will not induce such curvature, as in the example above. These reparameterizations are\ntypically of the following form: \n\nOriginal random effects parameterization:\n$$\n\\theta_{k} \\sim \\mbox{Normal}(\\theta, \\sigma)\n$$\nNew parameterization: \n\n$$\n\\theta_{k} = \\theta + \\sigma z_{k} \\mbox{  with } z_{k} \\sim \\mbox{Normal}(0, 1)\n$$\n \nA similar idea works if you have multivariate parameters, for instance in a varying-intercepts varying-slopes model. This time, let\n$\\theta_{k}$ be a vector of parameters: \n\nOriginal parameterization: \n$$\n\\theta_{k} \\sim \\mbox{Multi normal}(\\theta, \\Sigma)\n$$\nNew parameterization:\n$$\n\\theta_{k} = \\theta + \\mbox{Chol}(\\Sigma) z_{k} \\mbox{  with } \\mbox{vec}(z_{k}) \\sim \\mbox{Normal}(0, 1)\n$$\nHere, $\\mbox{Chol}(\\Sigma)$ is the Cholesky factor of $\\Sigma$. Cholesky factors are a sort of square root operator \nfor square invertable matrices. \n\n### Exercise 2: Panel data\n\nIn some recent research with Jeff Alstott (Media Lab, National Academy), we have been investigating whether the growth rates\nof technologies and the variation in their growth rates are related. One very simple model of the progress of technology $y_{i,t}$\nwith continuous compounding growth rate $g$ would be: \n\n$$\n\\log(y_{i, t}) = a_{i} + g_{i}t + \\epsilon_{i,t} \\mbox{ with } \\epsilon_{i,t} \\sim \\mbox{Normal}(0, \\sigma_{i})\n$$\nThe research question is whether there is a strong correlation between $\\sigma_{i}$ and $g_{i}$. \nTypically we will have, say, 10 observations of each technology (and for some, fewer), so we want to \nmake sure that our inference appropriately accounts for the small-data nature. Because the data are small, \nestimates of $a, g_{i}$ and $\\sigma$ will be noisy; if we can learn a good hyperprior for the model, \nwe'll be able to generate better predictions and inference. \n\nA data generating process for such a correlated structure might be: \n\n$$\n\\log(y_{i, t}) = a_{i} + g_{i}t + \\epsilon_{i,t} \\mbox{ with } \\epsilon_{i,t} \\sim \\mbox{Normal}(0, \\sigma_{i})\n$$\nwith \n\n$$\n(a_{i}, g_{i}, \\log(\\sigma_{i}))' \\sim \\mbox{Multi normal} \\left(\\mu, \\mbox{diag}(\\tau)\\Omega\\mbox{diag}(\\tau)\\right)\n$$\nwhere $\\mu$ is a vector of locations, $\\tau$ is a vector of scales, and $\\Omega$ is a correlation matrix. \n\nLet's simulate some data from this model: \n\n```{r, eval = F}\nlibrary(dplyr); library(ggplot2)\nset.seed(42)\nT <- 10 # of observations per technology\nJ <- 20 # number of technologies\ntau <- abs(rnorm(3))\nOmega <- matrix(c(1, -.5, 0, -.5, 1, .5, 0, .5, 1), 3, 3)\nSigma <- diag(tau)%*% Omega %*% diag(tau)\nmu <- c(1, 1, .3)\nsome_parameters <- as.data.frame(MASS::mvrnorm(J, mu, Sigma)) %>%\n  mutate(tech = 1:J,\n         sigma = exp(V3)) %>% \n  rename(a = V1, b = V2) %>% \n  select(-V3)\n\n# A data grid\ndata_grid <- expand.grid(tech = 1:J, time = 1:T) %>% \n  left_join(some_parameters) %>% \n  mutate(technology_log_level = rnorm(n(), a + b*time, sigma)) %>% \n  arrange(tech, time)\n\n\n# Have a look at the data\ndata_grid %>% \n  ggplot(aes(x = time, y = technology_log_level, group = tech)) +\n  geom_line()\n\n\n\n```\n\nNow, let's code up the model, precisely as we propose the data generating process to be\n\n```\n// saved as models/simple_panel_reparam.stan\ndata {\n  int N; // number of observations in total\n  int J; // number of technologies\n  vector[N] time; // time \n  int tech[N]; // tech index\n  vector[N] y; // the log levels of the technology\n}\nparameters {\n  matrix[J, 3] z;\n  vector[3] theta_mu;\n  vector<lower = 0>[3] theta_tau;\n  corr_matrix[3] Omega;\n}\ntransformed parameters {\n  matrix[J, 3] theta;\n  for(j in 1:J) {\n    theta[j] = (theta_mu + cholesky_decompose(quad_form_diag(Omega, theta_tau)) * z[j]')';\n  }\n}\nmodel {\n  theta_mu ~ normal(0, 1);\n  theta_tau ~ cauchy(0, 1);\n  Omega ~ lkj_corr(2);\n  \n  to_vector(z) ~ normal(0, 1);\n  \n  for(i in 1:N) {\n    y[i] ~ normal(theta[tech[i], 1] + theta[tech[i], 2]* time[i], exp(theta[tech[i], 3]));\n  }\n}\n\n```\n\n\nNow let's run it: \n\n```{r, eval = F}\ntech_mod <- stan_model(\"models/simple_panel_reparam.stan\")\ntest_tech <- sampling(tech_mod, data = list(N = nrow(data_grid), \n                                            J = J, time = data_grid$time,\n                                            tech = data_grid$tech, \n                                            y = data_grid$technology_log_level), iter = 500)\n\n# And let's look at our estimates\nget_posterior_mean(test_tech, \"theta\")[,5] %>% matrix(J, 3, byrow = T)\n\nprint(test_tech, \"theta_mu\")\n\nprint(test_tech, \"Omega\")\n```",
    "created" : 1484823015630.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2008656565",
    "id" : "83A990FC",
    "lastKnownWriteTime" : 1484916182,
    "last_content_update" : 1484916182471,
    "path" : "~/Documents/BSEcon/Econ short course/02-hierarchical_modeling_hierarchical_data.Rmd",
    "project_path" : "02-hierarchical_modeling_hierarchical_data.Rmd",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}