# An introduction to hierarchical modeling {#hierarchical}


### What is hierarchical modeling

Hierarchical modeling is the practice of building _rich_ models, typcially in which each individual in your dataset 
has their own set of parameters. Of course, without good prior information, this might not be identified, or might
be very weakly identified. Hierarchical modeling helps us deal with this problem by considering parameters at the low
level as "sharing" information across individuals. This structure is known as "partial pooling". This session covers
partial pooling, starting from the canonical example "8 schools", then shows how you can use partial pooling to provide
prior information when combining previous studies with a new dataset. Finally I show how partial pooling can be used 
for analysis of panel data. 

### Why do hierarchical modeling? 

There are a few excellent reasons to do hierarchical modeling:

**To deal with unobserved information fairly fixed at the level of the group**

The standard reason in economics to use panel data is to be able to "control for" confounding information that is fixed at the 
level of the individual over time. A similar motivation exists in hierarchical modeling. 

The big difference is that we will not consider the individual or time effects to be fixed. Indeed, we routinely 
"shrink" effects towards a group-level average. This encodes the heuristic "death, taxes, and mean reversion". Cross-validating
your results will almost always show that such an approach is superior to fixed effects for prediction. 

**Prediction with high-dimensional categorical variables**

Often in applied economics we have very high-dimensional categorical variables. For instance, plant, manager, project etc. 
This can massively increase the size of the parameter space, and result in over-fitting/poor generalization. In contrast, 
implementing high-dimensioned predictors as (shrunken) random effects typically results in large improvements in predictive
power. 

**Mr P: Multi-level regression and post-stratification**

As economists we want to make inferences, typically of a causal nature. A common problem is that our data are not 
collected randomly; we have some survey bias. Frequentists tend to correct for this by weighting observations according
to the inverse of their probability of being observed. Yet this approach moves away from a generative model, making 
model comparison and validation difficult. 

Mr P is the practice of fitting a model in which individuals, or groups of individuals (say, grouped by demographic cell)
have their own sets of parameters (which are shrunk towards a hierarchical prior). When we want to make an inference for a new population
we only need to know its demographics. The inference is the weighted average across effects in the sample, with weights coming
from the new population. 

This method has the advantage that we can work with highly-biased samples, while keeping within a generative framework (making
Modern Statistical Workflow completely doable). In a notorious example, Mr P was used by David Rothschild at Microsoft Research
to predict the outcome of the 2012 election based on a survey run through the Xbox platform. The survey was almost entirely young men. 




### Exchangeability

Astute readers familiar with fixed effects models will have noted a problem with one of my arguments above. I said that 
we could use random intercepts to soak up unobserved information that affects both $X$ and $y$ by including group-varying
intercepts $\alpha_{j}$. But this implies that the unobserved information fixed in a group, $\alpha_{j}$, is correlated with 
$X$. This correlation violates a very important rule in varying-intercept, varying-slope models: exchangeability. 

> Exchangeability says that there should be no information other than the outcome $y$ that should allow us to distinguish the 
group to which a group-level parameter belongs.

In this example, we can clearly use values of X to predict $j$, violating exchangeability. But all is not lost. The group-varying
parameter needs not be uncorrelated with X, _only the random portion of it_. 


### Conditional exchangeability and the Bafumi Gelman correction

Imagine we have an exchangability issue for a very simple model with only group-varying intercept: 
the unobserved information $\alpha_{j}$ is correlated with $X_{i,j}$ across groups. 
Let's break $\alpha_{j}$ down into its fixed and random portions. 

$$
\alpha_{j} = \mu_{1} + \eta_{j}
$$

where
$$
\eta_{j} \sim \mbox{normal}(0, \sigma_{\eta})
$$

So that now the regression model can be written as

$$
y_{i,t} = \mu_{1}  + X_{i,j}\beta + e_{i,j} \mbox{ where } e_{i,j} = \epsilon_{i,j}+ \eta_{j}
$$

For the correlation to hold, it must be the case that $\eta_{j}$ is correlated with $X_{i,j}$. But our regression error is 
$e_{i,j}$, which is clearly correlated with $X$ violating the Gauss-Markov theorem and so giving us biased estimates. 

In a [nice little paper](http://www.stat.columbia.edu/~gelman/research/unpublished/Bafumi_Gelman_Midwest06.pdf) Bafumi and Gelman suggest an elegant fix to this: simply control for group level averages in the model of $\alpha_{j}$. This is a Bayesian take 
on what econometricians might know as a Mundlak/Chamberlain approach. If $\bar{X}_{j}$ is the mean of $X_{i,j}$ in group $j$, 
then we could use the model 

$$
\alpha_{j} = \hat{\alpha} + \gamma \hat{X}_{j} + \nu_{j}
$$

which results in the correlaton between $\nu_{j}$ and $X_{i,j}$ across groups being 0. It's straightforward to implement, and
gets you to _conditional exchangeability_---a condition under which mixed models like this one are valid. 


### Exercise 1: Hierarchical priors

In this exercise we'll estimate an experimental treatment effect using linear
regression, while incorporating prior information from previous studies. Rather than doing this in stages 
(estimating the treatment effect and then doing some meta-analysis), we'll do everything in one pass. 
This has the advantage of helping us to get more precise estimates of all our model parameters. 

### A very basic underlying model

Let's say that we run the $J$'th experiment estimating the treatment effect of some treatment $x$ on an outcome $Y$. 
It's an expensive and ethically challenging experiment to run, so unfortunately we're only able to get a sample size of 20.
For simplicity, we can assume that the treatment has the same treatment effect for all people, $\theta$ (this is easily
dropped in more elaborate analyses). There have been $J-1$ similar experiments 
run in the past. In this example our outcome data $Y$ are conditionally normally distributed
with (untreated) mean $\mu$ and standard deviation $\sigma$. There is nothing to stop us from having a far
more complex model for the data. So the outcome model looks like this: 

$$
y_{i, J} \sim \mbox{Normal}(\mu + \theta_{J} x_{i,J}, \sigma)
$$

The question is: how can we estimate the parameters of this model while taking account of 
the information from the $J-1$ previous studies? The answer is to use the so-called _hierarchical prior_. 

### The hierarchical prior

Let's say that each of the $J-1$ previous studies each has an estimated treatment effect $\beta_{j}$, estimated 
with some standard error $se_{j}$. Taken together, are these estimates of $\beta_{j}$ the ground truth for the true 
treatment effect in their respective studies? One way of answering this is to ask ourselves: if the researchers
of each of those studies replicated their study in precisely the same way, but _after_ checking the estimates estimated 
by the other researchers, would they expect to find the same estimate they found before, $\beta_{j}$? Or would 
they perhaps expect some other treatment effect estimate, $\theta_{j}$, that balances the information from their own
study with the other studies? 

The answer to this question gives rise to the hierarchical prior. In this prior, we say that the estimated treatment effect $\beta$
is a noisy measure of the underlying treatment effect $\theta_{j}$ for each study $j$. These underlying effects
are in turn noisy estimates of the true average treatment effect $\hat{\theta}$---noisy because of uncontrolled-for varation across experiments. That is, if we make assumptions of normality:

$$
\beta_{j} \sim \mbox{Normal}(\theta_{j}, se_{j})
$$

and

$$
\theta_{j} \sim \mbox{Normal}\left(\hat{\theta}, \tau\right)
$$

Where $\tau$ is the standard devation of the distribution of plausible experimental estimates. 

The analysis therefore has the following steps: 

- Build a model of the treatment effects, considering our own study as another data point
- Jointly estimate the hyperdistribution of treatment effects. 

As an example, we'll take the original 8-schools data, with some fake data for the experiment we want to estimate. 
The 8-schools example comes from an education intervention modeled by Rubin, in which a similar experiment was 
conducted in 8 schools, with only treatment effects and their standard errors reported. The task is to generate
an estimate of the possible treatment effect that we might expect if we were to roll out the program across all schools. 

```{r, warning = F, message = F}
library(rstan); library(dplyr); library(ggplot2); library(reshape2)

# The original 8 schools data
schools_dat <- data_frame(beta = c(28,  8, -3,  7, -1,  1, 18, 12),
                          se = c(15, 10, 16, 11,  9, 11, 10, 18))

# The known parameters of our data generating process for fake data
mu <- 10
sigma <- 5
N <- 20
# Our fake treatment effect estimate drawn from the posterior of the 8 schools example
theta_J <- rnorm(1, 8, 6.45) 

# Create some fake data
treatment <- sample(0:1, N, replace = T)
y <- rnorm(N, mu + theta_J*treatment, sigma)
```

The Stan program we use to estimate the model is below. Note that these models can be difficult to fit, 
and so we employ a "reparameterization" below for the `theta`s. This is achieved by noticing that if 

$$
\theta_{j} \sim \mbox{Normal}\left(\hat{\theta}, \tau\right)
$$
then 
$$
\theta_{j} = \hat{\theta} + \tau z_{j}
$$

where $z_{j}\sim\mbox{Normal}(0,1)$. A standard normal has an easier geometry for Stan to work with, so
this parameterization of the model is typically preferred. Here is the Stan model:

```{r, eval = F}
// We save this as 8_schools_w_regression.stan
data {
  int<lower=0> J; // number of schools 
  int N; // number of observations in the regression problem
  real beta[J]; // estimated treatment effects from previous studies
  real<lower=0> se[J]; // s.e. of those effect estimates 
  vector[N] y; // the outcomes for students in our fake study data
  vector[N] treatment; // the treatment indicator in our fake study data
}
parameters {
  real mu; 
  real<lower=0> tau;
  real eta[J+1];
  real<lower = 0> sigma;
  real theta_hat;
}
transformed parameters {
  real theta[J+1];
  for (j in 1:(J+1)){
    theta[j] = theta_hat + tau * eta[j];
  }
}
model {
  // priors
  tau ~ cauchy(5, 2);
  mu ~ normal(10, 2);
  eta ~ normal(0, 1);
  sigma ~ cauchy(3, 3);
  theta_hat ~ normal(8, 5);
  
  // parameter model for previous studies
  for(j in 1:J) {
    beta[j] ~ normal(theta[j], se[j]);
  }
  
  // our regression
  y ~ normal(mu + theta[J+1]*treatment, sigma);
  
}

```

Now we estimate the model from R. Because of the geometry issues mentioned above, we use 
`control = list(adapt_delta = 0.99)` to prompt Stan to take smaller step sizes, improving 
sampling performance at a cost of slower estimation time (this isn't a problem here; it 
estimates in a couple of seconds). 

```{r, results = "hide", message = F, warning = F}
eight_schools_plus_regression <- stan("8_schools_w_regression.stan",
                       data = list(beta = schools_dat$beta,
                                   se = schools_dat$se,
                                   J = 8,
                                   y = y,
                                   N = N,
                                   treatment = treatment),
                       control = list(adapt_delta = 0.99))
```

Let's comapare the estimates we get for our regression model to those we might get from 
the Bayesian model. A simple linear regression model gives us the following confidence intervals
for the parameter estimates: 

```{r, echo = F, results = "asis", message = F, warning = F}
data_frame(coef = c("mu", "theta[9]"), estimates = coef(lm(y ~ treatment))) %>% bind_cols(confint(lm(y ~ treatment)) %>% as.data.frame) %>% pander::pander()
```

Our Bayesian model gives us more precise estimates for the treatment effect, with the 95% credibility region considerably smaller. 
This is because we have "borrowed"" information from the previous studies when estimating the treatment effect in the latest study. The estimates are also closer to the group-level mean.

```{r}
print(eight_schools_plus_regression, pars = c("mu", "theta[9]", "theta_hat"), probs = c(0.025, 0.5, 0.975))
```

### A note on reparameterizing

Hierarchical models are famous for inducing regions of high curvature in the typical set (see Betancourt 2017). Often, if we
implement these directly we get many divergent transitions, in which we cannot trust the results. We often use a reparameterization
to reshape the posterior into one that will not induce such curvature, as in the example above. These reparameterizations are
typically of the following form: 

Original random effects parameterization:
$$
\theta_{k} \sim \mbox{Normal}(\theta, \sigma)
$$
New parameterization: 

$$
\theta_{k} = \theta + \sigma z_{k} \mbox{  with } z_{k} \sim \mbox{Normal}(0, 1)
$$
 
A similar idea works if you have multivariate parameters, for instance in a varying-intercepts varying-slopes model. This time, let
$\theta_{k}$ be a vector of parameters: 

Original parameterization: 
$$
\theta_{k} \sim \mbox{Multi normal}(\theta, \Sigma)
$$
New parameterization:
$$
\theta_{k} = \theta + \mbox{Chol}(\Sigma) z_{k} \mbox{  with } \mbox{vec}(z_{k}) \sim \mbox{Normal}(0, 1)
$$
Here, $\mbox{Chol}(\Sigma)$ is the Cholesky factor of $\Sigma$. Cholesky factors are a sort of square root operator 
for square invertable matrices. 

### Exercise 2: Panel data

In some recent research with Jeff Alstott (Media Lab, National Academy), we have been investigating whether the growth rates
of technologies and the variation in their growth rates are related. One very simple model of the progress of technology $y_{i,t}$
with continuous compounding growth rate $g$ would be: 

$$
\log(y_{i, t}) = a_{i} + g_{i}t + \epsilon_{i,t} \mbox{ with } \epsilon_{i,t} \sim \mbox{Normal}(0, \sigma_{i})
$$
The research question is whether there is a strong correlation between $\sigma_{i}$ and $g_{i}$. 
Typically we will have, say, 10 observations of each technology (and for some, fewer), so we want to 
make sure that our inference appropriately accounts for the small-data nature. Because the data are small, 
estimates of $a, g_{i}$ and $\sigma$ will be noisy; if we can learn a good hyperprior for the model, 
we'll be able to generate better predictions and inference. 

A data generating process for such a correlated structure might be: 

$$
\log(y_{i, t}) = a_{i} + g_{i}t + \epsilon_{i,t} \mbox{ with } \epsilon_{i,t} \sim \mbox{Normal}(0, \sigma_{i})
$$
with 

$$
(a_{i}, g_{i}, \log(\sigma_{i}))' \sim \mbox{Multi normal} \left(\mu, \mbox{diag}(\tau)\Omega\mbox{diag}(\tau)\right)
$$
where $\mu$ is a vector of locations, $\tau$ is a vector of scales, and $\Omega$ is a correlation matrix. 

Let's simulate some data from this model: 

```{r, eval = F}
library(dplyr); library(ggplot2)
set.seed(42)
T <- 10 # of observations per technology
J <- 20 # number of technologies
tau <- abs(rnorm(3))
Omega <- matrix(c(1, -.5, 0, -.5, 1, .5, 0, .5, 1), 3, 3)
Sigma <- diag(tau)%*% Omega %*% diag(tau)
mu <- c(1, 1, .3)
some_parameters <- as.data.frame(MASS::mvrnorm(J, mu, Sigma)) %>%
  mutate(tech = 1:J,
         sigma = exp(V3)) %>% 
  rename(a = V1, b = V2) %>% 
  select(-V3)

# A data grid
data_grid <- expand.grid(tech = 1:J, time = 1:T) %>% 
  left_join(some_parameters) %>% 
  mutate(technology_log_level = rnorm(n(), a + b*time, sigma)) %>% 
  arrange(tech, time)


# Have a look at the data
data_grid %>% 
  ggplot(aes(x = time, y = technology_log_level, group = tech)) +
  geom_line()



```

Now, let's code up the model, precisely as we propose the data generating process to be

```
// saved as models/simple_panel_reparam.stan
data {
  int N; // number of observations in total
  int J; // number of technologies
  vector[N] time; // time 
  int tech[N]; // tech index
  vector[N] y; // the log levels of the technology
}
parameters {
  matrix[J, 3] z;
  vector[3] theta_mu;
  vector<lower = 0>[3] theta_tau;
  corr_matrix[3] Omega;
}
transformed parameters {
  matrix[J, 3] theta;
  for(j in 1:J) {
    theta[j] = (theta_mu + cholesky_decompose(quad_form_diag(Omega, theta_tau)) * z[j]')';
  }
}
model {
  theta_mu ~ normal(0, 1);
  theta_tau ~ cauchy(0, 1);
  Omega ~ lkj_corr(2);
  
  to_vector(z) ~ normal(0, 1);
  
  for(i in 1:N) {
    y[i] ~ normal(theta[tech[i], 1] + theta[tech[i], 2]* time[i], exp(theta[tech[i], 3]));
  }
}

```


Now let's run it: 

```{r, eval = F}
tech_mod <- stan_model("models/simple_panel_reparam.stan")
test_tech <- sampling(tech_mod, data = list(N = nrow(data_grid), 
                                            J = J, time = data_grid$time,
                                            tech = data_grid$tech, 
                                            y = data_grid$technology_log_level), iter = 500)

# And let's look at our estimates
get_posterior_mean(test_tech, "theta")[,5] %>% matrix(J, 3, byrow = T)

print(test_tech, "theta_mu")

print(test_tech, "Omega")
```